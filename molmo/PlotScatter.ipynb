{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48470430-fca3-4d9c-9931-d8d6d1631034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 14:24:40.609795: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-29 14:24:41.491119: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-29 14:24:41.491161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-29 14:24:41.491190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 14:24:41.628839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 14:25:02.667260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = Path.cwd().parent\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "from molmo_utils import load_model, do_inference, get_coordinates, calculate_probability_of_coordinates\n",
    "from utils import extract_all, plot_scatter, calculate_euclidian_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33330640-b4e5-4a52-b79f-90e10dac3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1f0816-c7da-431c-b8a7-affba3d892e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23780c93134f499ba0e2c9ff72ce42c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'allenai/Molmo-7B-D-0924'\n",
    "model, processor = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66be5bb1-e25d-48df-938c-215735b3d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_start = \"You are the robot in the picture. Your task is to: \"\n",
    "\n",
    "prompt_what_to_move = \" To complete this task, do one thing. 1. Find the relevant object and point to it.\"\n",
    "prompt_end_what_to_move = \" Your output format should be like this: (x1, y1) with (x1, y1) beeing the coordinates of the object to move. Dont output anything else.\"\n",
    "\n",
    "prompt_where_to_place = \" To complete this task, do one thing. 1. Point to the location where the relevant object should be moved to.\"\n",
    "prompt_end_where_to_place = \" Your output format should be like this: (x1, y1) with (x1, y1) beeing the coordinates where the relevant object should be moved to. Dont output anything else.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbbed0-99a2-49ba-a250-115bd1189f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: pull the oven tray\n",
      "Task: move the fruit from the left stove to the sink\n",
      "Task: open low fridge\n",
      "Task: open the drawer\n",
      "Task: Move the can from the top left of the burner to the bottom left of the burner\n",
      "Task: Place the pot to the right of the blue fork.\n",
      "Task: move the pot from the right to the left stove\n",
      "Task: put potato in pot or pan\n",
      "Task: take cucumber out of cup\n",
      "Task: move the banana from the left stove to the sink\n",
      "Task: pick up the toast and put it to the sink\n",
      "Task: close the microwave\n",
      "Task: push the toaster lever\n",
      "Task: pick up glass cup\n",
      "Task: take the broccoli and put it between the two right burners\n",
      "Task: move the pot from the left to the right stove\n"
     ]
    }
   ],
   "source": [
    "tasks, images, ground_truths, _ = extract_all(\"../data/base_experiments\")\n",
    "softmax_fn = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "distance_what = []\n",
    "dist_where = []\n",
    "\n",
    "prob_what = []\n",
    "prob_where = []\n",
    "\n",
    "distances_per_task = []\n",
    "probs_per_task = []\n",
    "for i in range(len(tasks)):\n",
    "    task = tasks[i]\n",
    "    ground_truth = ground_truths[i]\n",
    "    prompt_what = prompt_start + task + prompt_what_to_move + prompt_end_what_to_move\n",
    "    prompt_where = prompt_start + task + prompt_where_to_place + prompt_end_where_to_place\n",
    "    print(f\"Task: {task}\")\n",
    "\n",
    "    distances = [[],[]]\n",
    "    probs = [[],[]]\n",
    "\n",
    "    with Image.open(images[i]) as image:\n",
    "        image_width = image.width\n",
    "        image_height = image.height\n",
    "\n",
    "        prompts = [prompt_what]\n",
    "        if (len(ground_truth) == 2):\n",
    "            prompts.append(prompt_where)\n",
    "\n",
    "        for k in range(len(prompts)):\n",
    "            for j in range(5):\n",
    "                output_text, output, inputs = do_inference(image, prompts[k], model, processor)\n",
    "                \n",
    "                coordinates = get_coordinates(output_text, image_width, image_height)\n",
    "                if len(coordinates) == 1:\n",
    "                    euc_dist = calculate_euclidian_distance(coordinates[0], ground_truth[k])\n",
    "                    coord_probs = calculate_probability_of_coordinates(output, inputs, processor)\n",
    "                    \n",
    "                    distances[k].append(euc_dist)\n",
    "                    probs[k].append(coord_probs)\n",
    "\n",
    "    # Per task output\n",
    "    distances_per_task.append(distances[0] + distances[1])\n",
    "    probs_per_task.append(probs[0] + probs[1])\n",
    "\n",
    "    # Object to move / where to place output\n",
    "    distance_what.append(distances[0])\n",
    "    prob_what.append(probs[0])\n",
    "    if (distances[1] != []):\n",
    "        dist_where.append(distances[1])\n",
    "        prob_where.append(probs[1])\n",
    "    \n",
    "\n",
    "dists = [distance_what, dist_where]\n",
    "probs_2 = [prob_what, prob_where]\n",
    "        \n",
    "plot_scatter(tasks, probs_per_task, distances_per_task, \"Scatterplot for every task\")\n",
    "plot_scatter([\"Object to move\", \"Where to place\"], probs_2, dists, \"Scatterplot for difference between object and where to place\")\n",
    "\n",
    "# 2 labels - what to move, where to place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6c892-e82a-4e8d-92ce-156f445b5a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
